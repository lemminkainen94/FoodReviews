{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d22bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchnlp.encoders.text import StaticTokenizerEncoder, stack_and_pad_tensors, pad_tensor\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from time import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f2549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd98ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBSADataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          padding='max_length',\n",
    "          return_token_type_ids=False,\n",
    "          max_length=self.max_len,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation='only_first'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'review_text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd5a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df):\n",
    "    ds = TBSADataset(\n",
    "        texts=df.text.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=128\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=32,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded8b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_no_tweet.tsv', sep='\\t')\n",
    "df.sentiment += 1\n",
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.2,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "df_train, df_eval = train_test_split(\n",
    "  df_train,\n",
    "  test_size=0.2,\n",
    "  random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81dd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = create_data_loader(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9845bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = create_data_loader(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce6ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dl = create_data_loader(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "259fdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c62278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBSA(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TBSA, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained('google/electra-small-discriminator', return_dict=False)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.out = nn.Linear(self.transformer.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.transformer(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "        )[0][:, 0, :]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92de9b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12200/961378687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnum_warmup_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mnum_training_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "model = TBSA().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=int(len(train_dl) * 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff15bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dl, acc_steps=1):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    avg_losses = []\n",
    "    temp_preds = []\n",
    "    temp_targets = []\n",
    "    avg_accs = []\n",
    "    acc_losses = []\n",
    "    correct_predictions = 0\n",
    "    i = 0\n",
    "    t0 = time()\n",
    "    train_size = len(df_train)\n",
    "    for d in train_dl:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        targets = d[\"targets\"].to(device).view(-1)\n",
    "\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        preds = outputs.argmax(1, keepdim = True).view(-1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss = loss / acc_steps\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        temp_preds += preds.cpu().tolist()\n",
    "        temp_targets += targets.cpu().tolist()\n",
    "\n",
    "        acc_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        i += 1\n",
    "        if i % acc_steps == 0:\n",
    "            losses.append(np.mean(acc_losses))\n",
    "            acc_losses = []\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        if i % (100 * acc_steps) == 0:\n",
    "            acc = 0\n",
    "            try:\n",
    "                acc = accuracy_score(temp_targets, temp_preds)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            temp_preds = []\n",
    "            temp_targets = []\n",
    "\n",
    "            avg_accs.append(acc)\n",
    "            avg_losses.append(np.mean(losses[i-100:i]))\n",
    "            print(i, 'iters, auroc, loss, time : ', avg_accs[-1], avg_losses[-1], time()-t0)\n",
    "\n",
    "    return correct_predictions.double() / train_size, np.mean(losses), avg_losses, avg_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18abc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, eval_dl):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    temp_preds = []\n",
    "    temp_targets = []   \n",
    "    correct_predictions = 0\n",
    "    eval_size = len(df_eval)\n",
    "    with torch.no_grad():\n",
    "        for d in eval_dl:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            targets = d[\"targets\"].to(device).view(-1)\n",
    "            outputs = torch.zeros_like(targets)\n",
    "\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            preds = outputs.argmax(1, keepdim = True).view(-1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            temp_preds += preds.cpu().tolist()\n",
    "            temp_targets += targets.cpu().tolist()\n",
    "\n",
    "\n",
    "    acc = 0\n",
    "    try:\n",
    "        acc = accuracy_score(temp_targets, temp_preds)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return correct_predictions.double() / eval_size, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7549e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, test_dl):\n",
    "    model = model.eval()\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in test_dl:\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            preds = outputs.argmax(1, keepdim = True).view(-1)\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, real_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb8732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_acc = 0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72616686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(epoch, best_acc):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss, train_avg_losses, avg_accs = train_epoch(model, train_dl)\n",
    "    \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(model, eval_dl)\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'] += train_avg_losses\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "    return best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d83ee3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.7096875 0.7012013110518456 23.46341633796692\n",
      "Train loss 0.6748644018270931 accuracy 0.7217368961973278\n",
      "Val   loss 0.5442714143183923 accuracy 0.7882836587872559\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.8121875 0.4827631662786007 23.28145956993103\n",
      "Train loss 0.48346972502157337 accuracy 0.8111510791366907\n",
      "Val   loss 0.5431040544663707 accuracy 0.7882836587872559\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.80375 0.5017260302603245 23.578564882278442\n",
      "Train loss 0.495240148462233 accuracy 0.8060123329907503\n",
      "Val   loss 0.5385278895978005 accuracy 0.7882836587872559\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.8059375 0.4934819088876247 23.377612829208374\n",
      "Train loss 0.48639885287304396 accuracy 0.8101233299075026\n",
      "Val   loss 0.5365809346399 accuracy 0.7882836587872559\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.8153125 0.4842222927510738 23.436198234558105\n",
      "Train loss 0.4895379302687332 accuracy 0.8108941418293937\n",
      "Val   loss 0.5366865204226586 accuracy 0.7882836587872559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    run_epoch(epoch, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0362891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80936729663106\n"
     ]
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a0fb05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.685866444605009"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da5d74ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[186,  10,  66],\n",
       "       [ 49,  55,  51],\n",
       "       [ 44,  12, 744]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cab89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_text'] = df.apply(lambda x: x.text + ' [SEP] ' + x.target, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610ee6e",
   "metadata": {},
   "source": [
    "# TBSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a3b75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tb_data_loader(df):\n",
    "    ds = TBSADataset(\n",
    "        texts=df.target_text.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=128\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=32,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "219b5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.2,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "df_train, df_eval = train_test_split(\n",
    "  df_train,\n",
    "  test_size=0.2,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "train_dl = create_tb_data_loader(df_train)\n",
    "\n",
    "test_dl = create_tb_data_loader(df_test)\n",
    "\n",
    "eval_dl = create_tb_data_loader(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c8007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#weights = torch.Tensor(compute_class_weight('balanced', [0,1,2], df_train.sentiment.values))\n",
    "#weights = weights / weights.sum()\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "model = TBSA().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n",
    "\n",
    "#scheduler = get_linear_schedule_with_warmup(\n",
    "#    optimizer,\n",
    "#    num_warmup_steps=0,\n",
    "#    num_training_steps=int(len(train_dl) * 1)\n",
    "#)\n",
    "history = defaultdict(list)\n",
    "best_acc = 0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "884bdde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.94375 0.1646930056065321 6.757266521453857\n",
      "Train loss 0.16879604820955973 accuracy 0.9427029804727647\n",
      "Val   loss 0.6755291050480258 accuracy 0.828365878725591\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.955625 0.12535545540042223 6.445020914077759\n",
      "Train loss 0.13122159748750387 accuracy 0.9524665981500514\n",
      "Val   loss 0.6031383843191208 accuracy 0.8324768756423433\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.9628125 0.10862114432267844 6.639143705368042\n",
      "Train loss 0.11192483143102316 accuracy 0.9617163412127441\n",
      "Val   loss 0.7191731285664343 accuracy 0.8263103802672148\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.97 0.089422884255182 6.57964825630188\n",
      "Train loss 0.08958865710740267 accuracy 0.9707091469681398\n",
      "Val   loss 0.743628466321576 accuracy 0.8437821171634121\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "100 iters, auroc, loss, time :  0.9778125 0.07244041429134086 6.425522804260254\n",
      "Train loss 0.07732532101064982 accuracy 0.9763617677286742\n",
      "Val   loss 0.7573038868365749 accuracy 0.8386433710174718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    run_epoch(epoch, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9ea504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8348397699260477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7507956592076185"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch=128\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f94cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8373048479868529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.752758858452138"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch=64\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d803536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8233360723089564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7414560150462578"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#256\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3737d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8134757600657354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7300259866812592"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#192\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb3b6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojtek\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8356614626129828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7674470928633134"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#128\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c3b53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8027937551355793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7024931239464111"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#96\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7cc65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175842235004108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7256547521288529"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#64\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d63fd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[197,  15,  50],\n",
       "       [ 33,  78,  44],\n",
       "       [ 55,  28, 717]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c42b893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8299096138044372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7577898094622112"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#128\n",
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea6a561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8307313064913723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.751533864377337"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d33e5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8364831552999178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.752528986417904"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_test = get_predictions(model, test_dl)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1b40ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojtek\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0, 1, 2], y=[2 2 2 ... 2 2 2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.3627451 , 2.51421189, 0.53520352])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_class_weight('balanced', [0,1,2], df_train.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07e2478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3089, 0.5698, 0.1213])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights / weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa4e98",
   "metadata": {},
   "source": [
    "# Using the interface we-ve developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cfbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval import eval_model, get_predictions\n",
    "from src.transformer_model import TransformerTBSA\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from types import SimpleNamespace\n",
    "from src.data_loader import create_data_loader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355fdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace()\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.final_dropout = 0.2\n",
    "args.model_name = 'yangheng/deberta-v3-base-absa-v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b8566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/food_reviews_occ.tsv', sep='\\t')\n",
    "df.sentiment += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482c20ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model = TransformerTBSA(args)\n",
    "args.model.load_state_dict(torch.load('models/transformer_deberta-v3-base-absa-v1.1_32_5e-05.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49241ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 32\n",
    "args.max_len = 128\n",
    "args.lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3e3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b68dcb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojtek\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df['target_text'] = df.apply(lambda x: x.text + ' [SEP] ' + x.target, axis=1)\n",
    "\n",
    "args.tokenizer = AutoTokenizer.from_pretrained(args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d1f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.test_dl = create_data_loader(df, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f505d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de8ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model.to(args.device)\n",
    "y_review_texts, y_pred, y_test = get_predictions(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bd942c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.733745402430063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03626fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e64de339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['we ordered two omelettes and two lemonades.', 1, 2],\n",
       "       [\"sorry but i don't want french fries with my omelette.\", 1, 0],\n",
       "       ['after half an hour we ask where our order is, the waitress that it will be in 10 minutes, then after 15 minutes we ask again and that they are \"already putting on\" ... frustrated after another 5 minutes we get up and say that we are leaving, the waitress brings food suddenly (already not the one who served us originally, the other one has already started avoiding us), only an omelette was served, the second course was served after another 5 minutes ... we ate quickly and paid equally quickly and left.',\n",
       "        0, 1],\n",
       "       [\"we didn't eat the omelette to the end and we ended up visiting ... it's a pity but i won't use it anymore\",\n",
       "        1, 2],\n",
       "       ['failure ... a long waiting period, 45 minutes for breakfast (fried eggs and an omelette is a bit of a lot) plus winter coffee, because the waitress forgot to bring ... dinner supposedly ok, but portions are small compared to the price and vegetables are undercooked, the service is messy and not very nice, i do not recommend it',\n",
       "        1, 2],\n",
       "       ['Slightly dry toast, omelette could be better seasoned.', 0, 1],\n",
       "       ['perfect pizza if you like to go to the toilet every now and then',\n",
       "        2, 1],\n",
       "       ['average pizza, a bit burnt, the waiting time was supposed to be 20 minutes and it was 45. the staff did not even know if i had settled the payment.',\n",
       "        1, 2],\n",
       "       [\"pizza ok. the nagner did not give a quiet read menu, we entered because they had a blanket and fire and in january evening a big + after we entered the waiter-nagner disappeared when i went inside to place the order because there was no waiter, 3-4 tables were occupied after ordering beer and pizza we've been waiting for pizza almost 40 minutes ...\",\n",
       "        1, 2],\n",
       "       ['pizza is good to try', 1, 2],\n",
       "       ['carrots, sweet salad, sweet ukrainian borscht.', 1, 0],\n",
       "       ['i have always appreciated aioli, a very nice place with a kitchen that suits my taste, but after the last visit i was a bit disappointed when it comes to prices ... basic burger 37 zlotys, caesar salad with chicken 42 zlotys, freshly squeezed juice ... diluted with water, pilsner beer 17 zlotys.. as long as the price of the burger can be forgiven, because it was tasty and with additives, the price of the salad is a cosmos, a few leaves of lettuce and not all 100g of chicken for pln 42.',\n",
       "        1, 2],\n",
       "       ['nice to start the restaurant with goma wakame seaweed salad.',\n",
       "        0, 1],\n",
       "       ['the apple salad was okay (3/5) but the rabbit dumplings were not very good.',\n",
       "        2, 1],\n",
       "       ['first class food, mmmm salad, heaven in mouth chicken 🤩🥰 very nice service.',\n",
       "        0, 1],\n",
       "       ['pad thai with salty tofu, as if covered with vinegar.', 0, 1],\n",
       "       ['i will risk saying that here is the best pad thai in gdańsk.',\n",
       "        0, 1],\n",
       "       ['food delicious, beef ramen and chicken pad thai were great!', 2,\n",
       "        1],\n",
       "       ['old and seasoned chicken.', 0, 1],\n",
       "       ['a big disadvantage, if the menu with meat is only chicken breast and veal brisket, it is unacceptable that one of these dishes is missing, and so it happened that the brisket is missing.',\n",
       "        0, 1],\n",
       "       ['everything on the menu is the same - supreme chicken, burgers and steaks….',\n",
       "        0, 1],\n",
       "       ['delicious liver and chicken, a revelation fish soup.', 1, 2],\n",
       "       ['i chose those with chicken and pulled pork.', 1, 2],\n",
       "       ['pulled pork was my favorite though.', 1, 0],\n",
       "       [\"pulled pork sandwich, served with marinated vegetables and barbecue sauce - one of the best dishes i've ever had.\",\n",
       "        2, 1],\n",
       "       ['green curry is a box full of oil.....really unhealthy and bad....',\n",
       "        0, 1],\n",
       "       ['green chicken curry also nice, rich in additives.', 0, 1],\n",
       "       ['the rest did it - spring rolls, curry chicken, ribs ok. not outstanding, but tasty.',\n",
       "        1, 0],\n",
       "       ['the yellow curry was also polonized, but tasty nonetheless, and it was 3 stars.. as for the prices of 2x pho soup and 2x curry, it was around pln 120.',\n",
       "        1, 0],\n",
       "       ['this place is at a great location the staff here are very nice and friendly.i am vegan so you have  food for everyone.i had the tofu bao set and my wife had the mango curry.plus we ordered for drinks and sweet potato fries.all of it was very good.the place has good atmosphere and they are very quick with the food.overall i would highly recommend this place.',\n",
       "        1, 0],\n",
       "       ['laughable lasagne, do you serve the same on site, or just for takeaway?',\n",
       "        1, 0],\n",
       "       ['lasagne was microwaved.', 1, 0],\n",
       "       ['very delicious lasagne, great service!', 0, 1],\n",
       "       [\"wife's best lasagne ever she said.\", 1, 0],\n",
       "       ['we ate lasagne and carbonare, something lovely!', 0, 2],\n",
       "       ['we were not disappointed either, because the cauliflower risotto and the mutton burger were very delicious and nicely served.',\n",
       "        1, 0],\n",
       "       ['big wow for the tartare and the tuna one and the deer one ... the seafood risotto is a masterpiece.',\n",
       "        1, 2],\n",
       "       ['steaks are nothing but fat, just terrible', 1, 0],\n",
       "       ['the steaks are gone .. the fish .. today we just left the restaurant after a few minutes of studying the menu.',\n",
       "        1, 2],\n",
       "       ['a very climatic place, good dishes (delicious pasta and steaks), prices - know, quite high but you pay for prestige.',\n",
       "        0, 2],\n",
       "       ['great steak', 1, 0],\n",
       "       ['after a long wait i got a dry kebab with no sauce in a burnt tortilla.',\n",
       "        1, 2],\n",
       "       ['the homemade tortillas are delicious though, but the place seems to be popping mostly because of its location and big terrasse, especially during sunny days.',\n",
       "        2, 1],\n",
       "       ['check out the pecan pie, and those hand made tortillas!', 1, 2],\n",
       "       ['fish & chips dry and hard, blue tartare, dirty in the bathroom, cables sticking out of the walls.',\n",
       "        0, 2],\n",
       "       ['i think steaks can be good, but for fish and chips it is 3 plus.',\n",
       "        1, 0],\n",
       "       ['i have never eaten such a good fish!. fish soup also very tasty, well seasoned.',\n",
       "        1, 2],\n",
       "       ['very good fish and other dishes, affordable prices and adequate for the quality and location',\n",
       "        1, 2],\n",
       "       ['we wanted something a bit more homely, with fresh fish on offer and a relaxed atmosphere.',\n",
       "        0, 1],\n",
       "       ['the fish is very good, the soup even more so, generous portions at a moderate price.',\n",
       "        1, 2],\n",
       "       ['tasty tripe but as for the main course, it is a pity that i ordered them, i had pork tenderloin with blanched vegetables, they were cauliflower, carrots and broccoli, it is a pity that at this time of the year the cook uses frozen vegetables, on top of that they were overcooked summer and tasteless and the pancakes were also, unfortunately, frozen and extra soaked fry',\n",
       "        0, 2],\n",
       "       ['we ordered typically breakfast food french toast and my daughter had pancakes.. my daughter pancakes were cold.',\n",
       "        0, 1],\n",
       "       ['there were only 3 potato pancakes.', 1, 0],\n",
       "       ['food presented very nice, i highly recommend the nachos... are waiter chris aka fred was outstanding... well worth the visit... will be returning for pancakes...',\n",
       "        1, 2],\n",
       "       ['we went for breakfasts (full english, beef bagel, pancakes).',\n",
       "        0, 1],\n",
       "       [\"both of our mains (the ravioli and the steak ) were very average and underwhelming.. i could get over that if the quality was above par, but i've seriously made store bought ravioli and pasta sauce that tasted better.\",\n",
       "        2, 1],\n",
       "       ['different impressions because the ravioli are delicious and the cod with chips was clearly frozen, dripping with fat and terribly over-salt, the waiting time for food was very long.',\n",
       "        0, 1],\n",
       "       ['very good pasta and ravioli only quite high prices 😌', 0, 2],\n",
       "       ['pepper-mushroom soup is great and delicious, but a bit drenched in hundreds of pepper balls in one bowl',\n",
       "        2, 1],\n",
       "       ['delicious kimchi bowl!', 1, 2],\n",
       "       ['in my life i have never eaten seafood or beef worse.', 0, 1],\n",
       "       ['beef tartare ok, while boar bacon is average at most', 1, 0],\n",
       "       ['we went for breakfasts (full english, beef bagel, pancakes).',\n",
       "        0, 1],\n",
       "       ['first time to have beef cheeks, the meat was tasty tender and juicy!',\n",
       "        0, 1],\n",
       "       ['rubbery seafood ...', 2, 1],\n",
       "       ['dish: tagliatelle with seafood, delivery.', 0, 1],\n",
       "       ['only seafood dishes.', 2, 1],\n",
       "       ['the seafood risotto and spaghetti bolognese were really good.',\n",
       "        1, 0],\n",
       "       ['nice service, atmospheric place, 1st class drinks and seafood delicious!',\n",
       "        0, 1],\n",
       "       ['there was nothing else in the falafel wrap, except dry lettuce, two slices of cucumber, and i found one red onion ring.. and uninteresting falafels.',\n",
       "        0, 2],\n",
       "       ['delicious falafels and power bowls.', 0, 1],\n",
       "       ['very average ramen, not to say weak, too hot, cold, poor additions for this price, i do not recommend it.',\n",
       "        1, 0],\n",
       "       ['to sum up… ramen very average, fat with water, meat is a misunderstanding….',\n",
       "        1, 2],\n",
       "       ['ramen is great, want to try other dishes!', 1, 2],\n",
       "       ['burgers, waffles and ramen rock!', 1, 0],\n",
       "       ['great ramen.', 0, 1],\n",
       "       ['bao buns came out especially fast, most likely skipping corners, the beef looked old and was cold',\n",
       "        1, 2],\n",
       "       ['great service, burger very weak, fish pesto ok, bao buns ... average.',\n",
       "        0, 2],\n",
       "       [\"really really really bad personel, bad atmosphere, very looooong time of waiting for everything, cold inside and the worst thing is that burger was expensive and also really bad - long story short everything here was bad and i don't recommend at all.\",\n",
       "        0, 1],\n",
       "       ['we ordered a burger, the meat stank and the taste was broken.',\n",
       "        0, 2],\n",
       "       ['unfortunately, for the last few months the menu has been limited to items like .. um .. i would say bar-style .. burgers appeared, for example.',\n",
       "        1, 2],\n",
       "       ['i have always appreciated aioli, a very nice place with a kitchen that suits my taste, but after the last visit i was a bit disappointed when it comes to prices ... basic burger 37 zlotys, caesar salad with chicken 42 zlotys, freshly squeezed juice ... diluted with water, pilsner beer 17 zlotys.. as long as the price of the burger can be forgiven, because it was tasty and with additives, the price of the salad is a cosmos, a few leaves of lettuce and not all 100g of chicken for pln 42.',\n",
       "        0, 1],\n",
       "       ['burger - average, mostly cold, other dishes have defended themselves.',\n",
       "        0, 1],\n",
       "       ['i came with my friends for a burger, they all had time to eat and i waited and waited (35 minutes).. the burger itself is great',\n",
       "        1, 2],\n",
       "       ['pear and parsley soup was inedible, desserts that were once insane at this point are a failure.',\n",
       "        1, 0],\n",
       "       [\"my mother and i went to the restaurant to taste coffee and dessert, we didn't try the dishes.\",\n",
       "        1, 2],\n",
       "       ['without the sauce the ribs are anything, then with my hands full of sauce for the ribs it lasted half an hour that they came to attend to me to order dessert (the only good thing about the place) they never gave me napkins.',\n",
       "        1, 0],\n",
       "       ['amazing food, both drinks and desserts.', 1, 0],\n",
       "       ['the waiter offered dessert as an apology.. a very large dessert!',\n",
       "        1, 2],\n",
       "       ['tasteless sushi, the rolls were falling apart', 1, 0],\n",
       "       ['tasteless chicken ramen, disappointing sushi.', 1, 2],\n",
       "       ['i am very disappointed, i ordered sushi for delivery, the quality of the products is not the best or fresh, the tempura shrimp rolls were soaked, all the rolls were falling off dry reams and i had the impression that it was sour.. i do not recommend it, the worst sushi i have ever eaten ...',\n",
       "        1, 0],\n",
       "       [\"roy's serves delicious sushi rolls.\", 1, 2],\n",
       "       ['the scrambled eggs was overcooked and it seemes the chef must have been in a rush.',\n",
       "        1, 2],\n",
       "       ['i just had a simple breakfast: scrambled eggs with bread + butter.',\n",
       "        1, 2],\n",
       "       ['delicious scrambled eggs with bacon and chives to which they give fresh, crispy bread.',\n",
       "        0, 1],\n",
       "       ['delicious breakfasts, scrambled eggs, omelettes, pancakes, toasts, the choice is wide and everyone will like something',\n",
       "        1, 2],\n",
       "       [\"we haven't eaten such a bad soup for a long time :) literally diluted broth with a small amount of pasta, tasteless.\",\n",
       "        1, 0],\n",
       "       ['tomato cream soup - mixed soup, no revelation.', 0, 1],\n",
       "       ['food very well presented, unfortunately wild boar ribs, sour rye soup and fondant very average, meringue on the other hand delicious.',\n",
       "        0, 1],\n",
       "       ['tasty food, delicious sour rye soup, very good cod.', 1, 2],\n",
       "       [\"very nice restaurant great decor, there is a kids' corner and food cossack tomato cream 10/10 will be eaten many times recommend prices git onion soup also extra and mushroom tagiatelle wonderful such a creamy mmhmm people go there\",\n",
       "        1, 0],\n",
       "       [\" friendly service, food generally very good, except for the usual hummus, which is served with some main courses - very watery texture, poorly seasoned, it rarely happens that something really does not taste good to me and that was unfortunately such a situation :( i don't know if it was because of the consistency of the dish, or maybe for other reasons, i felt sick for a few hours after the meal\",\n",
       "        1, 0],\n",
       "       ['recommendable hummus, shakshuka and both types of breakfast - meat and vegetarian.',\n",
       "        0, 1],\n",
       "       ['i tried the black spaghetti, gnocci, steak.', 1, 0],\n",
       "       ['are food finally arrived in my father-in-law who had gotten the salmon had a fairly over cooked fish, i had gotten the scallop dish with three of the smallest scallops i have ever seen, my mother-in-law had the risotto which was very watery, and my husband had the pork and spaghetti dish which was very bland.',\n",
       "        1, 0],\n",
       "       ['spaghetti with meatballs - \"sauce\" looked like someone just poured canned tomatoes over the pasta, the whole thing looked unappetizing.',\n",
       "        2, 1],\n",
       "       [\"my daughter really liked the ravioli,  but the rest of us weren't that impressed with the spaghetti,  fettuccine, or mostaccoili  it was fine italian food, but it wasn't anything we hadn't had before or made at home.\",\n",
       "        2, 1],\n",
       "       ['very good shrimp spaghetti', 0, 1],\n",
       "       ['spaghetti with sirloin and molto delicioso porcini mushrooms',\n",
       "        0, 1],\n",
       "       ['the seafood spaghetti was brilliant.', 1, 2],\n",
       "       ['food lumpy, potato pancakes very good, dumplings sticking together badly, so there was a lot of water inside.',\n",
       "        1, 2],\n",
       "       ['pretty tasty dumplings.. unfortunately, the waiting time from the declared 15 minutes was 35. wonton soup inedible - such water after dumplings with 3 tablespoons of salt.',\n",
       "        1, 2],\n",
       "       [\"i am forced to change my opinion, after the last visit i was surprised by a different menu, the fact is that i haven't been there for a long time, but i remember the fried dumplings well.. currently, there are no fried dumplings on the menu.. the dumplings themselves tasty, but earlier it was better.\",\n",
       "        1, 2],\n",
       "       ['coffee is so so, but other things are good.', 1, 2],\n",
       "       ['however, the coffee is delicious.', 1, 2],\n",
       "       ['but the coffee is good.', 1, 0]], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.column_stack([df.text.iloc[np.where(y_test != y_pred)].values, \n",
    "                y_test[np.where(y_test != y_pred)],\n",
    "               y_pred[np.where(y_test != y_pred)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881ec7c",
   "metadata": {},
   "source": [
    "Looks like most of the mistakes are debatable. Luckily, it handles the positive comments nicely, so it should be pretty accurate combined with the beta distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d120dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stars</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omelette</td>\n",
       "      <td>we ordered two omelettes and two lemonades.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>we ordered two omelettes and two lemonades. [S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>omelette</td>\n",
       "      <td>delicious breakfasts, scrambled eggs, omelette...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>delicious breakfasts, scrambled eggs, omelette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omelette</td>\n",
       "      <td>we took a baguette and an omelette with paprik...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>we took a baguette and an omelette with paprik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omelette</td>\n",
       "      <td>always a good spot for some delightful breakfa...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>always a good spot for some delightful breakfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omelette</td>\n",
       "      <td>my omelette and grits were better than expected.</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>my omelette and grits were better than expecte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text  sentiment  \\\n",
       "0  omelette        we ordered two omelettes and two lemonades.          1   \n",
       "1  omelette  delicious breakfasts, scrambled eggs, omelette...          2   \n",
       "2  omelette  we took a baguette and an omelette with paprik...          2   \n",
       "3  omelette  always a good spot for some delightful breakfa...          2   \n",
       "4  omelette   my omelette and grits were better than expected.          2   \n",
       "\n",
       "   stars                                        target_text  \n",
       "0      1  we ordered two omelettes and two lemonades. [S...  \n",
       "1      5  delicious breakfasts, scrambled eggs, omelette...  \n",
       "2      5  we took a baguette and an omelette with paprik...  \n",
       "3      5  always a good spot for some delightful breakfa...  \n",
       "4      4  my omelette and grits were better than expecte...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ea2217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8874388254486134"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_pred_bin = (y_pred > 0).to(int)\n",
    "y_bin = (y_test > 0).to(int)\n",
    "f1_score(y_bin, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e9000bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759581881533101"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bin = (y_pred < 1).to(int)\n",
    "y_bin = (y_test < 1).to(int)\n",
    "f1_score(y_bin, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4da44b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTBSA(\n",
       "  (transformer): DebertaV2ForSequenceClassification(\n",
       "    (deberta): DebertaV2Model(\n",
       "      (embeddings): DebertaV2Embeddings(\n",
       "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): StableDropout()\n",
       "      )\n",
       "      (encoder): DebertaV2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (1): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (2): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (3): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (4): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (5): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (6): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (7): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (8): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (9): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (10): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (11): DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rel_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): ContextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untrained model:\n",
    "args.model = TransformerTBSA(args)\n",
    "args.model.to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e0ddc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_test = get_predictions(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09dc301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6635894678729262"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "501c21a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644444444444444"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa85cb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7456\\1655619513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/transformer_electra-small-discriminator_32_5e-05.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_review_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\FoodReviews\\src\\eval.py\u001b[0m in \u001b[0;36mget_predictions\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"targets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "args.model_name = 'google/electra-small-discriminator'\n",
    "args.model = TransformerTBSA(args)\n",
    "args.model.load_state_dict(torch.load('models/transformer_electra-small-discriminator_32_5e-05.pt'))\n",
    "args.model.to(args.device)\n",
    "y_review_texts, y_pred, y_test = get_predictions(args)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10a53a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13200\\3054707579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformerTBSA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/transformer_distilbert-base-uncased_32_5e-05.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_review_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[1;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[1;32m-> 1001\u001b[1;33m             \u001b[0mwrap_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m             dtype=dtype)\n\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;31m# del _CudaBase.__new__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "args.model_name = 'distilbert-base-uncased'\n",
    "args.model = TransformerTBSA(args)\n",
    "args.model.load_state_dict(torch.load('models/transformer_distilbert-base-uncased_32_5e-05.pt'))\n",
    "args.model.to(args.device)\n",
    "y_review_texts, y_pred, y_test = get_predictions(args)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d41b342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6766876805581461"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name = 'yangheng/deberta-v3-base-absa-v1.1'\n",
    "args.model = TransformerTBSA(args)\n",
    "args.model.load_state_dict(torch.load('models/transformer_1671719673.6458306.pt'))\n",
    "args.model.to(args.device)\n",
    "y_review_texts, y_pred, y_test = get_predictions(args)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c669bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
